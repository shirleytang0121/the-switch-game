{"ast":null,"code":"var AWS = require('../core');\n\nvar byteLength = AWS.util.string.byteLength;\nvar Buffer = AWS.util.Buffer;\n/**\r\n * The managed uploader allows for easy and efficient uploading of buffers,\r\n * blobs, or streams, using a configurable amount of concurrency to perform\r\n * multipart uploads where possible. This abstraction also enables uploading\r\n * streams of unknown size due to the use of multipart uploads.\r\n *\r\n * To construct a managed upload object, see the {constructor} function.\r\n *\r\n * ## Tracking upload progress\r\n *\r\n * The managed upload object can also track progress by attaching an\r\n * 'httpUploadProgress' listener to the upload manager. This event is similar\r\n * to {AWS.Request~httpUploadProgress} but groups all concurrent upload progress\r\n * into a single event. See {AWS.S3.ManagedUpload~httpUploadProgress} for more\r\n * information.\r\n *\r\n * ## Handling Multipart Cleanup\r\n *\r\n * By default, this class will automatically clean up any multipart uploads\r\n * when an individual part upload fails. This behavior can be disabled in order\r\n * to manually handle failures by setting the `leavePartsOnError` configuration\r\n * option to `true` when initializing the upload object.\r\n *\r\n * @!event httpUploadProgress(progress)\r\n *   Triggered when the uploader has uploaded more data.\r\n *   @note The `total` property may not be set if the stream being uploaded has\r\n *     not yet finished chunking. In this case the `total` will be undefined\r\n *     until the total stream size is known.\r\n *   @note This event will not be emitted in Node.js 0.8.x.\r\n *   @param progress [map] An object containing the `loaded` and `total` bytes\r\n *     of the request and the `key` of the S3 object. Note that `total` may be undefined until the payload\r\n *     size is known.\r\n *   @context (see AWS.Request~send)\r\n */\n\nAWS.S3.ManagedUpload = AWS.util.inherit({\n  /**\r\n   * Creates a managed upload object with a set of configuration options.\r\n   *\r\n   * @note A \"Body\" parameter is required to be set prior to calling {send}.\r\n   * @option options params [map] a map of parameters to pass to the upload\r\n   *   requests. The \"Body\" parameter is required to be specified either on\r\n   *   the service or in the params option.\r\n   * @note ContentMD5 should not be provided when using the managed upload object.\r\n   *   Instead, setting \"computeChecksums\" to true will enable automatic ContentMD5 generation\r\n   *   by the managed upload object.\r\n   * @option options queueSize [Number] (4) the size of the concurrent queue\r\n   *   manager to upload parts in parallel. Set to 1 for synchronous uploading\r\n   *   of parts. Note that the uploader will buffer at most queueSize * partSize\r\n   *   bytes into memory at any given time.\r\n   * @option options partSize [Number] (5mb) the size in bytes for each\r\n   *   individual part to be uploaded. Adjust the part size to ensure the number\r\n   *   of parts does not exceed {maxTotalParts}. See {minPartSize} for the\r\n   *   minimum allowed part size.\r\n   * @option options leavePartsOnError [Boolean] (false) whether to abort the\r\n   *   multipart upload if an error occurs. Set to true if you want to handle\r\n   *   failures manually.\r\n   * @option options service [AWS.S3] an optional S3 service object to use for\r\n   *   requests. This object might have bound parameters used by the uploader.\r\n   * @option options tags [Array<map>] The tags to apply to the uploaded object.\r\n   *   Each tag should have a `Key` and `Value` keys.\r\n   * @example Creating a default uploader for a stream object\r\n   *   var upload = new AWS.S3.ManagedUpload({\r\n   *     params: {Bucket: 'bucket', Key: 'key', Body: stream}\r\n   *   });\r\n   * @example Creating an uploader with concurrency of 1 and partSize of 10mb\r\n   *   var upload = new AWS.S3.ManagedUpload({\r\n   *     partSize: 10 * 1024 * 1024, queueSize: 1,\r\n   *     params: {Bucket: 'bucket', Key: 'key', Body: stream}\r\n   *   });\r\n   * @example Creating an uploader with tags\r\n   *   var upload = new AWS.S3.ManagedUpload({\r\n   *     params: {Bucket: 'bucket', Key: 'key', Body: stream},\r\n   *     tags: [{Key: 'tag1', Value: 'value1'}, {Key: 'tag2', Value: 'value2'}]\r\n   *   });\r\n   * @see send\r\n   */\n  constructor: function ManagedUpload(options) {\n    var self = this;\n    AWS.SequentialExecutor.call(self);\n    self.body = null;\n    self.sliceFn = null;\n    self.callback = null;\n    self.parts = {};\n    self.completeInfo = [];\n\n    self.fillQueue = function () {\n      self.callback(new Error('Unsupported body payload ' + typeof self.body));\n    };\n\n    self.configure(options);\n  },\n\n  /**\r\n   * @api private\r\n   */\n  configure: function configure(options) {\n    options = options || {};\n    this.partSize = this.minPartSize;\n    if (options.queueSize) this.queueSize = options.queueSize;\n    if (options.partSize) this.partSize = options.partSize;\n    if (options.leavePartsOnError) this.leavePartsOnError = true;\n\n    if (options.tags) {\n      if (!Array.isArray(options.tags)) {\n        throw new Error('Tags must be specified as an array; ' + typeof options.tags + ' provided.');\n      }\n\n      this.tags = options.tags;\n    }\n\n    if (this.partSize < this.minPartSize) {\n      throw new Error('partSize must be greater than ' + this.minPartSize);\n    }\n\n    this.service = options.service;\n    this.bindServiceObject(options.params);\n    this.validateBody();\n    this.adjustTotalBytes();\n  },\n\n  /**\r\n   * @api private\r\n   */\n  leavePartsOnError: false,\n\n  /**\r\n   * @api private\r\n   */\n  queueSize: 4,\n\n  /**\r\n   * @api private\r\n   */\n  partSize: null,\n\n  /**\r\n   * @readonly\r\n   * @return [Number] the minimum number of bytes for an individual part\r\n   *   upload.\r\n   */\n  minPartSize: 1024 * 1024 * 5,\n\n  /**\r\n   * @readonly\r\n   * @return [Number] the maximum allowed number of parts in a multipart upload.\r\n   */\n  maxTotalParts: 10000,\n\n  /**\r\n   * Initiates the managed upload for the payload.\r\n   *\r\n   * @callback callback function(err, data)\r\n   *   @param err [Error] an error or null if no error occurred.\r\n   *   @param data [map] The response data from the successful upload:\r\n   *     * `Location` (String) the URL of the uploaded object\r\n   *     * `ETag` (String) the ETag of the uploaded object\r\n   *     * `Bucket` (String) the bucket to which the object was uploaded\r\n   *     * `Key` (String) the key to which the object was uploaded\r\n   * @example Sending a managed upload object\r\n   *   var params = {Bucket: 'bucket', Key: 'key', Body: stream};\r\n   *   var upload = new AWS.S3.ManagedUpload({params: params});\r\n   *   upload.send(function(err, data) {\r\n   *     console.log(err, data);\r\n   *   });\r\n   */\n  send: function send(callback) {\n    var self = this;\n    self.failed = false;\n\n    self.callback = callback || function (err) {\n      if (err) throw err;\n    };\n\n    var runFill = true;\n\n    if (self.sliceFn) {\n      self.fillQueue = self.fillBuffer;\n    } else if (AWS.util.isNode()) {\n      var Stream = AWS.util.stream.Stream;\n\n      if (self.body instanceof Stream) {\n        runFill = false;\n        self.fillQueue = self.fillStream;\n        self.partBuffers = [];\n        self.body.on('error', function (err) {\n          self.cleanup(err);\n        }).on('readable', function () {\n          self.fillQueue();\n        }).on('end', function () {\n          self.isDoneChunking = true;\n          self.numParts = self.totalPartNumbers;\n          self.fillQueue.call(self);\n\n          if (self.isDoneChunking && self.totalPartNumbers >= 1 && self.doneParts === self.numParts) {\n            self.finishMultiPart();\n          }\n        });\n      }\n    }\n\n    if (runFill) self.fillQueue.call(self);\n  },\n\n  /**\r\n   * @!method  promise()\r\n   *   Returns a 'thenable' promise.\r\n   *\r\n   *   Two callbacks can be provided to the `then` method on the returned promise.\r\n   *   The first callback will be called if the promise is fulfilled, and the second\r\n   *   callback will be called if the promise is rejected.\r\n   *   @callback fulfilledCallback function(data)\r\n   *     Called if the promise is fulfilled.\r\n   *     @param data [map] The response data from the successful upload:\r\n   *       `Location` (String) the URL of the uploaded object\r\n   *       `ETag` (String) the ETag of the uploaded object\r\n   *       `Bucket` (String) the bucket to which the object was uploaded\r\n   *       `Key` (String) the key to which the object was uploaded\r\n   *   @callback rejectedCallback function(err)\r\n   *     Called if the promise is rejected.\r\n   *     @param err [Error] an error or null if no error occurred.\r\n   *   @return [Promise] A promise that represents the state of the upload request.\r\n   *   @example Sending an upload request using promises.\r\n   *     var upload = s3.upload({Bucket: 'bucket', Key: 'key', Body: stream});\r\n   *     var promise = upload.promise();\r\n   *     promise.then(function(data) { ... }, function(err) { ... });\r\n   */\n\n  /**\r\n   * Aborts a managed upload, including all concurrent upload requests.\r\n   * @note By default, calling this function will cleanup a multipart upload\r\n   *   if one was created. To leave the multipart upload around after aborting\r\n   *   a request, configure `leavePartsOnError` to `true` in the {constructor}.\r\n   * @note Calling {abort} in the browser environment will not abort any requests\r\n   *   that are already in flight. If a multipart upload was created, any parts\r\n   *   not yet uploaded will not be sent, and the multipart upload will be cleaned up.\r\n   * @example Aborting an upload\r\n   *   var params = {\r\n   *     Bucket: 'bucket', Key: 'key',\r\n   *     Body: new Buffer(1024 * 1024 * 25) // 25MB payload\r\n   *   };\r\n   *   var upload = s3.upload(params);\r\n   *   upload.send(function (err, data) {\r\n   *     if (err) console.log(\"Error:\", err.code, err.message);\r\n   *     else console.log(data);\r\n   *   });\r\n   *\r\n   *   // abort request in 1 second\r\n   *   setTimeout(upload.abort.bind(upload), 1000);\r\n   */\n  abort: function abort() {\n    this.cleanup(AWS.util.error(new Error('Request aborted by user'), {\n      code: 'RequestAbortedError',\n      retryable: false\n    }));\n  },\n\n  /**\r\n   * @api private\r\n   */\n  validateBody: function validateBody() {\n    var self = this;\n    self.body = self.service.config.params.Body;\n\n    if (typeof self.body === 'string') {\n      self.body = new AWS.util.Buffer(self.body);\n    } else if (!self.body) {\n      throw new Error('params.Body is required');\n    }\n\n    self.sliceFn = AWS.util.arraySliceFn(self.body);\n  },\n\n  /**\r\n   * @api private\r\n   */\n  bindServiceObject: function bindServiceObject(params) {\n    params = params || {};\n    var self = this; // bind parameters to new service object\n\n    if (!self.service) {\n      self.service = new AWS.S3({\n        params: params\n      });\n    } else {\n      var service = self.service;\n      var config = AWS.util.copy(service.config);\n      config.signatureVersion = service.getSignatureVersion();\n      self.service = new service.constructor.__super__(config);\n      self.service.config.params = AWS.util.merge(self.service.config.params || {}, params);\n    }\n  },\n\n  /**\r\n   * @api private\r\n   */\n  adjustTotalBytes: function adjustTotalBytes() {\n    var self = this;\n\n    try {\n      // try to get totalBytes\n      self.totalBytes = byteLength(self.body);\n    } catch (e) {} // try to adjust partSize if we know payload length\n\n\n    if (self.totalBytes) {\n      var newPartSize = Math.ceil(self.totalBytes / self.maxTotalParts);\n      if (newPartSize > self.partSize) self.partSize = newPartSize;\n    } else {\n      self.totalBytes = undefined;\n    }\n  },\n\n  /**\r\n   * @api private\r\n   */\n  isDoneChunking: false,\n\n  /**\r\n   * @api private\r\n   */\n  partPos: 0,\n\n  /**\r\n   * @api private\r\n   */\n  totalChunkedBytes: 0,\n\n  /**\r\n   * @api private\r\n   */\n  totalUploadedBytes: 0,\n\n  /**\r\n   * @api private\r\n   */\n  totalBytes: undefined,\n\n  /**\r\n   * @api private\r\n   */\n  numParts: 0,\n\n  /**\r\n   * @api private\r\n   */\n  totalPartNumbers: 0,\n\n  /**\r\n   * @api private\r\n   */\n  activeParts: 0,\n\n  /**\r\n   * @api private\r\n   */\n  doneParts: 0,\n\n  /**\r\n   * @api private\r\n   */\n  parts: null,\n\n  /**\r\n   * @api private\r\n   */\n  completeInfo: null,\n\n  /**\r\n   * @api private\r\n   */\n  failed: false,\n\n  /**\r\n   * @api private\r\n   */\n  multipartReq: null,\n\n  /**\r\n   * @api private\r\n   */\n  partBuffers: null,\n\n  /**\r\n   * @api private\r\n   */\n  partBufferLength: 0,\n\n  /**\r\n   * @api private\r\n   */\n  fillBuffer: function fillBuffer() {\n    var self = this;\n    var bodyLen = byteLength(self.body);\n\n    if (bodyLen === 0) {\n      self.isDoneChunking = true;\n      self.numParts = 1;\n      self.nextChunk(self.body);\n      return;\n    }\n\n    while (self.activeParts < self.queueSize && self.partPos < bodyLen) {\n      var endPos = Math.min(self.partPos + self.partSize, bodyLen);\n      var buf = self.sliceFn.call(self.body, self.partPos, endPos);\n      self.partPos += self.partSize;\n\n      if (byteLength(buf) < self.partSize || self.partPos === bodyLen) {\n        self.isDoneChunking = true;\n        self.numParts = self.totalPartNumbers + 1;\n      }\n\n      self.nextChunk(buf);\n    }\n  },\n\n  /**\r\n   * @api private\r\n   */\n  fillStream: function fillStream() {\n    var self = this;\n    if (self.activeParts >= self.queueSize) return;\n    var buf = self.body.read(self.partSize - self.partBufferLength) || self.body.read();\n\n    if (buf) {\n      self.partBuffers.push(buf);\n      self.partBufferLength += buf.length;\n      self.totalChunkedBytes += buf.length;\n    }\n\n    if (self.partBufferLength >= self.partSize) {\n      // if we have single buffer we avoid copyfull concat\n      var pbuf = self.partBuffers.length === 1 ? self.partBuffers[0] : Buffer.concat(self.partBuffers);\n      self.partBuffers = [];\n      self.partBufferLength = 0; // if we have more than partSize, push the rest back on the queue\n\n      if (pbuf.length > self.partSize) {\n        var rest = pbuf.slice(self.partSize);\n        self.partBuffers.push(rest);\n        self.partBufferLength += rest.length;\n        pbuf = pbuf.slice(0, self.partSize);\n      }\n\n      self.nextChunk(pbuf);\n    }\n\n    if (self.isDoneChunking && !self.isDoneSending) {\n      // if we have single buffer we avoid copyfull concat\n      pbuf = self.partBuffers.length === 1 ? self.partBuffers[0] : Buffer.concat(self.partBuffers);\n      self.partBuffers = [];\n      self.partBufferLength = 0;\n      self.totalBytes = self.totalChunkedBytes;\n      self.isDoneSending = true;\n\n      if (self.numParts === 0 || pbuf.length > 0) {\n        self.numParts++;\n        self.nextChunk(pbuf);\n      }\n    }\n\n    self.body.read(0);\n  },\n\n  /**\r\n   * @api private\r\n   */\n  nextChunk: function nextChunk(chunk) {\n    var self = this;\n    if (self.failed) return null;\n    var partNumber = ++self.totalPartNumbers;\n\n    if (self.isDoneChunking && partNumber === 1) {\n      var params = {\n        Body: chunk\n      };\n\n      if (this.tags) {\n        params.Tagging = this.getTaggingHeader();\n      }\n\n      var req = self.service.putObject(params);\n      req._managedUpload = self;\n      req.on('httpUploadProgress', self.progress).send(self.finishSinglePart);\n      return null;\n    } else if (self.service.config.params.ContentMD5) {\n      var err = AWS.util.error(new Error('The Content-MD5 you specified is invalid for multi-part uploads.'), {\n        code: 'InvalidDigest',\n        retryable: false\n      });\n      self.cleanup(err);\n      return null;\n    }\n\n    if (self.completeInfo[partNumber] && self.completeInfo[partNumber].ETag !== null) {\n      return null; // Already uploaded this part.\n    }\n\n    self.activeParts++;\n\n    if (!self.service.config.params.UploadId) {\n      if (!self.multipartReq) {\n        // create multipart\n        self.multipartReq = self.service.createMultipartUpload();\n        self.multipartReq.on('success', function (resp) {\n          self.service.config.params.UploadId = resp.data.UploadId;\n          self.multipartReq = null;\n        });\n        self.queueChunks(chunk, partNumber);\n        self.multipartReq.on('error', function (err) {\n          self.cleanup(err);\n        });\n        self.multipartReq.send();\n      } else {\n        self.queueChunks(chunk, partNumber);\n      }\n    } else {\n      // multipart is created, just send\n      self.uploadPart(chunk, partNumber);\n    }\n  },\n\n  /**\r\n   * @api private\r\n   */\n  getTaggingHeader: function getTaggingHeader() {\n    var kvPairStrings = [];\n\n    for (var i = 0; i < this.tags.length; i++) {\n      kvPairStrings.push(AWS.util.uriEscape(this.tags[i].Key) + '=' + AWS.util.uriEscape(this.tags[i].Value));\n    }\n\n    return kvPairStrings.join('&');\n  },\n\n  /**\r\n   * @api private\r\n   */\n  uploadPart: function uploadPart(chunk, partNumber) {\n    var self = this;\n    var partParams = {\n      Body: chunk,\n      ContentLength: AWS.util.string.byteLength(chunk),\n      PartNumber: partNumber\n    };\n    var partInfo = {\n      ETag: null,\n      PartNumber: partNumber\n    };\n    self.completeInfo[partNumber] = partInfo;\n    var req = self.service.uploadPart(partParams);\n    self.parts[partNumber] = req;\n    req._lastUploadedBytes = 0;\n    req._managedUpload = self;\n    req.on('httpUploadProgress', self.progress);\n    req.send(function (err, data) {\n      delete self.parts[partParams.PartNumber];\n      self.activeParts--;\n\n      if (!err && (!data || !data.ETag)) {\n        var message = 'No access to ETag property on response.';\n\n        if (AWS.util.isBrowser()) {\n          message += ' Check CORS configuration to expose ETag header.';\n        }\n\n        err = AWS.util.error(new Error(message), {\n          code: 'ETagMissing',\n          retryable: false\n        });\n      }\n\n      if (err) return self.cleanup(err);\n      partInfo.ETag = data.ETag;\n      self.doneParts++;\n\n      if (self.isDoneChunking && self.doneParts === self.numParts) {\n        self.finishMultiPart();\n      } else {\n        self.fillQueue.call(self);\n      }\n    });\n  },\n\n  /**\r\n   * @api private\r\n   */\n  queueChunks: function queueChunks(chunk, partNumber) {\n    var self = this;\n    self.multipartReq.on('success', function () {\n      self.uploadPart(chunk, partNumber);\n    });\n  },\n\n  /**\r\n   * @api private\r\n   */\n  cleanup: function cleanup(err) {\n    var self = this;\n    if (self.failed) return; // clean up stream\n\n    if (typeof self.body.removeAllListeners === 'function' && typeof self.body.resume === 'function') {\n      self.body.removeAllListeners('readable');\n      self.body.removeAllListeners('end');\n      self.body.resume();\n    } // cleanup multipartReq listeners\n\n\n    if (self.multipartReq) {\n      self.multipartReq.removeAllListeners('success');\n      self.multipartReq.removeAllListeners('error');\n      self.multipartReq.removeAllListeners('complete');\n      delete self.multipartReq;\n    }\n\n    if (self.service.config.params.UploadId && !self.leavePartsOnError) {\n      self.service.abortMultipartUpload().send();\n    } else if (self.leavePartsOnError) {\n      self.isDoneChunking = false;\n    }\n\n    AWS.util.each(self.parts, function (partNumber, part) {\n      part.removeAllListeners('complete');\n      part.abort();\n    });\n    self.activeParts = 0;\n    self.partPos = 0;\n    self.numParts = 0;\n    self.totalPartNumbers = 0;\n    self.parts = {};\n    self.failed = true;\n    self.callback(err);\n  },\n\n  /**\r\n   * @api private\r\n   */\n  finishMultiPart: function finishMultiPart() {\n    var self = this;\n    var completeParams = {\n      MultipartUpload: {\n        Parts: self.completeInfo.slice(1)\n      }\n    };\n    self.service.completeMultipartUpload(completeParams, function (err, data) {\n      if (err) {\n        return self.cleanup(err);\n      }\n\n      if (data && typeof data.Location === 'string') {\n        data.Location = data.Location.replace(/%2F/g, '/');\n      }\n\n      if (Array.isArray(self.tags)) {\n        self.service.putObjectTagging({\n          Tagging: {\n            TagSet: self.tags\n          }\n        }, function (e, d) {\n          if (e) {\n            self.callback(e);\n          } else {\n            self.callback(e, data);\n          }\n        });\n      } else {\n        self.callback(err, data);\n      }\n    });\n  },\n\n  /**\r\n   * @api private\r\n   */\n  finishSinglePart: function finishSinglePart(err, data) {\n    var upload = this.request._managedUpload;\n    var httpReq = this.request.httpRequest;\n    var endpoint = httpReq.endpoint;\n    if (err) return upload.callback(err);\n    data.Location = [endpoint.protocol, '//', endpoint.host, httpReq.path].join('');\n    data.key = this.request.params.Key; // will stay undocumented\n\n    data.Key = this.request.params.Key;\n    data.Bucket = this.request.params.Bucket;\n    upload.callback(err, data);\n  },\n\n  /**\r\n   * @api private\r\n   */\n  progress: function progress(info) {\n    var upload = this._managedUpload;\n\n    if (this.operation === 'putObject') {\n      info.part = 1;\n      info.key = this.params.Key;\n    } else {\n      upload.totalUploadedBytes += info.loaded - this._lastUploadedBytes;\n      this._lastUploadedBytes = info.loaded;\n      info = {\n        loaded: upload.totalUploadedBytes,\n        total: upload.totalBytes,\n        part: this.params.PartNumber,\n        key: this.params.Key\n      };\n    }\n\n    upload.emit('httpUploadProgress', [info]);\n  }\n});\nAWS.util.mixin(AWS.S3.ManagedUpload, AWS.SequentialExecutor);\n/**\r\n * @api private\r\n */\n\nAWS.S3.ManagedUpload.addPromisesToClass = function addPromisesToClass(PromiseDependency) {\n  this.prototype.promise = AWS.util.promisifyMethod('send', PromiseDependency);\n};\n/**\r\n * @api private\r\n */\n\n\nAWS.S3.ManagedUpload.deletePromisesFromClass = function deletePromisesFromClass() {\n  delete this.prototype.promise;\n};\n\nAWS.util.addPromises(AWS.S3.ManagedUpload);\n/**\r\n * @api private\r\n */\n\nmodule.exports = AWS.S3.ManagedUpload;","map":null,"metadata":{},"sourceType":"script"}